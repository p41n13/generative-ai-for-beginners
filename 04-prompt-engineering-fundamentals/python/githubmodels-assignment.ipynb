{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'J',\n",
       " b'upiter',\n",
       " b' is',\n",
       " b' the',\n",
       " b' fifth',\n",
       " b' planet',\n",
       " b' from',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b' and',\n",
       " b' the',\n",
       " b' largest',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' a',\n",
       " b' gas',\n",
       " b' giant',\n",
       " b' with',\n",
       " b' a',\n",
       " b' mass',\n",
       " b' one',\n",
       " b'-th',\n",
       " b'ousand',\n",
       " b'th',\n",
       " b' that',\n",
       " b' of',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b',',\n",
       " b' but',\n",
       " b' two',\n",
       " b'-and',\n",
       " b'-a',\n",
       " b'-half',\n",
       " b' times',\n",
       " b' that',\n",
       " b' of',\n",
       " b' all',\n",
       " b' the',\n",
       " b' other',\n",
       " b' planets',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b' combined',\n",
       " b'.',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' one',\n",
       " b' of',\n",
       " b' the',\n",
       " b' brightest',\n",
       " b' objects',\n",
       " b' visible',\n",
       " b' to',\n",
       " b' the',\n",
       " b' naked',\n",
       " b' eye',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b',',\n",
       " b' and',\n",
       " b' has',\n",
       " b' been',\n",
       " b' known',\n",
       " b' to',\n",
       " b' ancient',\n",
       " b' civilizations',\n",
       " b' since',\n",
       " b' before',\n",
       " b' recorded',\n",
       " b' history',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' named',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Roman',\n",
       " b' god',\n",
       " b' Jupiter',\n",
       " b'.[',\n",
       " b'19',\n",
       " b']',\n",
       " b' When',\n",
       " b' viewed',\n",
       " b' from',\n",
       " b' Earth',\n",
       " b',',\n",
       " b' Jupiter',\n",
       " b' can',\n",
       " b' be',\n",
       " b' bright',\n",
       " b' enough',\n",
       " b' for',\n",
       " b' its',\n",
       " b' reflected',\n",
       " b' light',\n",
       " b' to',\n",
       " b' cast',\n",
       " b' visible',\n",
       " b' shadows',\n",
       " b',[',\n",
       " b'20',\n",
       " b']',\n",
       " b' and',\n",
       " b' is',\n",
       " b' on',\n",
       " b' average',\n",
       " b' the',\n",
       " b' third',\n",
       " b'-b',\n",
       " b'right',\n",
       " b'est',\n",
       " b' natural',\n",
       " b' object',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Moon',\n",
       " b' and',\n",
       " b' Venus',\n",
       " b'.\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate Github Models Key Setup\n",
    "\n",
    "Run the code below to verify that your Github Models endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-inference\n",
      "  Downloading azure_ai_inference-1.0.0b6-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-ai-inference)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from azure-ai-inference) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/generative-ai-for-beginners/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2024.12.14)\n",
      "Downloading azure_ai_inference-1.0.0b6-py3-none-any.whl (115 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: isodate, azure-core, azure-ai-inference\n",
      "Successfully installed azure-ai-inference-1.0.0b6 azure-core-1.32.0 isodate-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, say can you see  \n",
      "By the dawn's early light,  \n",
      "What so proudly we hailed  \n",
      "At the twilight's last gleaming?  \n",
      "\n",
      "Is this the *Star-Spangled Banner* you're referencing? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, client, model_name, temperature=1.0, max_tokens=1000, top_p=1.0):\n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt, client, model_name)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there may be a mix-up or fictional element in your request, as there is no historical record of a \"Martian War of 2076.\" If you're looking for a lesson plan based on a fictional concept like this (perhaps for a creative writing class, speculative history, or science fiction workshop), I can help create an imaginative and engaging lesson plan. Let me know if this works for you!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt, client, model_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the fifth planet from the Sun and the biggest planet in our Solar System. It is made of gas and is much larger than all the other planets combined, but much smaller than the Sun. People have known about Jupiter for a very long time, even before they started writing things down. It is named after a Roman god. At night, Jupiter is very bright and can sometimes be the third-brightest thing in the sky, after the Moon and Venus.\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt, client, model_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, just in the glamorous hotspot of... Arlington, Texas. Yes, because nothing screams \"World Series\" like a neutral site in the middle of a pandemic! Globe Life Field became the Dodgers' adopted home away from home. Truly the stuff of legends.\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
