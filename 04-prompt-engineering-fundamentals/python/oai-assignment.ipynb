{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n",
      "Number of tokens: 135\n",
      "[b'\\n', b'J', b'upiter', b' is', b' the', b' fifth', b' planet', b' from', b' the', b' Sun', b' and', b' the', b' largest', b' in', b' the', b' Solar', b' System', b'.', b' It', b' is', b' a', b' gas', b' giant', b' with', b' a', b' mass', b' one', b'-th', b'ousand', b'th', b' that', b' of', b' the', b' Sun', b',', b' but', b' two', b'-and', b'-a', b'-half', b' times', b' that', b' of', b' all', b' the', b' other', b' planets', b' in', b' the', b' Solar', b' System', b' combined', b'.', b' Jupiter', b' is', b' one', b' of', b' the', b' brightest', b' objects', b' visible', b' to', b' the', b' naked', b' eye', b' in', b' the', b' night', b' sky', b',', b' and', b' has', b' been', b' known', b' to', b' ancient', b' civilizations', b' since', b' before', b' recorded', b' history', b'.', b' It', b' is', b' named', b' after', b' the', b' Roman', b' god', b' Jupiter', b'.[', b'19', b']', b' When', b' viewed', b' from', b' Earth', b',', b' Jupiter', b' can', b' be', b' bright', b' enough', b' for', b' its', b' reflected', b' light', b' to', b' cast', b' visible', b' shadows', b',[', b'20', b']', b' and', b' is', b' on', b' average', b' the', b' third', b'-b', b'right', b'est', b' natural', b' object', b' in', b' the', b' night', b' sky', b' after', b' the', b' Moon', b' and', b' Venus', b'.\\n']\n",
      "Average length of tokens: 4.67\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "decoded_tokens = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
    "print(decoded_tokens)\n",
    "\n",
    "# Calculate the average length of tokens\n",
    "total_length = sum(len(token) for token in decoded_tokens)\n",
    "average_length = total_length / len(decoded_tokens) if len(decoded_tokens) > 0 else 0\n",
    "\n",
    "# Print the average length of tokens\n",
    "print(f\"Average length of tokens: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the dawn's early light\n"
     ]
    }
   ],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable in your GitHub Codespace secrets.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\"\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Martian War of 2076: A Lesson in Interplanetary Conflict\n",
      "\n",
      "Objective: Students will analyze the causes and consequences of the Martian War of 2076, and evaluate the impact of interplanetary conflict on Earth and Mars.\n",
      "\n",
      "Materials:\n",
      "- Textbook or online resources on the Martian War of 2076\n",
      "- Maps of Mars and Earth\n",
      "- Writing materials\n",
      "- Computer or tablet for research\n",
      "\n",
      "Lesson Plan:\n",
      "\n",
      "1. Introduction (10 minutes)\n",
      "- Begin the lesson by asking students what they know about Mars and its relationship with Earth.\n",
      "- Introduce the Martian War of 2076 as a fictional event that will be the focus of the lesson.\n",
      "- Explain that students will be exploring the causes, events, and consequences of the war.\n",
      "\n",
      "2. Causes of the Martian War (15 minutes)\n",
      "- Divide students into small groups and have them research the possible causes of the Martian War of 2076.\n",
      "- Encourage students to consider factors such as resource scarcity, political tensions, and technological advancements.\n",
      "- Have each group present their findings to the class and facilitate a discussion on the various causes.\n",
      "\n",
      "3. Events of the Martian War (20 minutes)\n",
      "- Provide students with a timeline of key events during the Martian War of 2076.\n",
      "- Have students work individually or in pairs to create a visual representation of the timeline, including major battles and diplomatic negotiations.\n",
      "- Discuss the significance of each event and how they contributed to the outcome of the war.\n",
      "\n",
      "4. Consequences of the Martian War (15 minutes)\n",
      "- Ask students to consider the consequences of the Martian War on both Earth and Mars.\n",
      "- Have students write a short essay or create a presentation on the economic, social, and political impacts of the war on each planet.\n",
      "- Facilitate a class discussion on the long-term effects of interplanetary conflict.\n",
      "\n",
      "5. Reflection and Discussion (10 minutes)\n",
      "- Lead a class discussion on the ethical implications of interplanetary conflict and the importance of diplomacy in resolving disputes.\n",
      "- Encourage students to reflect on what they have learned about the Martian War of 2076 and how it relates to current events on Earth.\n",
      "- Conclude the lesson by asking students to share their thoughts on the significance of studying fictional conflicts in understanding real-world issues.\n",
      "\n",
      "6. Assessment:\n",
      "- Evaluate students based on their participation in group discussions, presentations, and written assignments.\n",
      "- Assess students' understanding of the causes, events, and consequences of the Martian War of 2076 through a quiz or essay.\n",
      "\n",
      "Extension Activity:\n",
      "- Have students research and present on other fictional interplanetary conflicts in literature or film, comparing and contrasting them with the Martian War of 2076.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is a big planet that is far away from the Sun. It is the biggest planet in our Solar System and is made of gas. People have known about Jupiter for a long time because it is very bright in the sky. It is named after a Roman god. Jupiter is so bright that sometimes you can see shadows from its light.\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearly, it was played in outer space. Where else would the 2020 World Series be held?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
