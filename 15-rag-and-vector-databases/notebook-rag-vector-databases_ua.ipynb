{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ—à—É–∫—É (RAG) —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mistralai getenv openai faiss-cpu pandas numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞—à–æ—ó –±–∞–∑–∏ –∑–Ω–∞–Ω—å\n",
    "\n",
    "–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è FAISS –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ª—è—Ö–∏ –¥–æ —Ñ–∞–π–ª—ñ–≤ (–¥–∞–Ω–∏—Ö –¥–ª—è RAG)\n",
    "data_paths = [\n",
    "    \"data/frameworks.md\", \n",
    "    \"data/own_framework.md\", \n",
    "    \"data/perceptron.md\"\n",
    "] \n",
    "\n",
    "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ DataFrame\n",
    "df = pd.DataFrame(columns=['path', 'text'])\n",
    "\n",
    "# –°—É—á–∞—Å–Ω–∏–π —Å–ø–æ—Å—ñ–± –¥–æ–¥–∞–≤–∞–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –¥–æ DataFrame\n",
    "for path in data_paths:\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "        \n",
    "        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ concat –∑–∞–º—ñ—Å—Ç—å –∑–∞—Å—Ç–∞—Ä—ñ–ª–æ–≥–æ append\n",
    "        new_row = pd.DataFrame({'path': [path], 'text': [file_content]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {path}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length, min_length):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    # –Ø–∫—â–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–µ –¥–æ—Å—è–≥–Ω—É–≤ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏, –≤—Å–µ –æ–¥–Ω–æ –¥–æ–¥–∞—Ç–∏ –π–æ–≥–æ\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# –ü—Ä–∏–ø—É—Å–∫–∞—é—á–∏, —â–æ analyzed_df - —Ü–µ pandas DataFrame, –∞ 'output_content' - —Ü–µ —Å—Ç–æ–≤–ø–µ—Ü—å —É —Ü—å–æ–º—É DataFrame\n",
    "splitted_df = df.copy()\n",
    "splitted_df['chunks'] = splitted_df['text'].apply(lambda x: split_text(x, 400, 300))\n",
    "\n",
    "splitted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–ø—É—Å–∫–∞—é—á–∏, —â–æ 'chunks' - —Ü–µ —Å—Ç–æ–≤–ø–µ—Ü—å —Å–ø–∏—Å–∫—ñ–≤ —É DataFrame splitted_df, –º–∏ —Ä–æ–∑–¥—ñ–ª–∏–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –Ω–∞ —Ä—ñ–∑–Ω—ñ —Ä—è–¥–∫–∏\n",
    "flattened_df = splitted_df.explode('chunks')\n",
    "\n",
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞—à–æ–≥–æ —Ç–µ–∫—Å—Ç—É –Ω–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∏\n",
    "\n",
    "–ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞—à–æ–≥–æ —Ç–µ–∫—Å—Ç—É –Ω–∞ –µ–º–±–µ–¥—ñ–Ω–≥–∏ —ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —Å–µ—Ä–≤—ñ—Å—É [MistralAI](https:/docs.mistral.ai/capabilities/embeddings/) —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —ó—Ö –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç–∞ Mistral\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "assert api_key, \"ERROR: MISTRAL_API_KEY is missing\"\n",
    "\n",
    "client_embedding = Mistral(api_key=api_key)\n",
    "\n",
    "def create_embeddings(text, model=\"mistral-embed\"):\n",
    "    \"\"\"\n",
    "    –°—Ç–≤–æ—Ä—é—î –µ–º–±–µ–¥—ñ–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ Mistral AI.\n",
    "    \n",
    "    Args:\n",
    "        text: –¢–µ–∫—Å—Ç –∞–±–æ —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è –µ–º–±–µ–¥—ñ–Ω–≥—É\n",
    "        model: –ú–æ–¥–µ–ª—å –¥–ª—è –µ–º–±–µ–¥—ñ–Ω–≥—É (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º mistral-embed)\n",
    "        \n",
    "    Returns:\n",
    "        –í–µ–∫—Ç–æ—Ä –µ–º–±–µ–¥—ñ–Ω–≥—É\n",
    "    \"\"\"\n",
    "    # –û–±—Ä–æ–±–∫–∞ pandas Series\n",
    "    if isinstance(text, pd.Series):\n",
    "        # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–∏–π –µ–ª–µ–º–µ–Ω—Ç –∑ Series\n",
    "        text = text.iloc[0]\n",
    "    \n",
    "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤ –¥–ª—è API\n",
    "    if not isinstance(text, list):\n",
    "        text = [str(text)]\n",
    "    else:\n",
    "        text = [str(item) for item in text]\n",
    "    \n",
    "    # –í–∏–∫–ª–∏–∫ API Mistral –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤\n",
    "    embeddings_response = client_embedding.embeddings.create(\n",
    "        model=model,\n",
    "        inputs=text\n",
    "    )\n",
    "    \n",
    "    # –ü–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—É –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞\n",
    "    return embeddings_response.data[0].embedding\n",
    "\n",
    "# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:\n",
    "embeddings = create_embeddings(flattened_df['chunks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = create_embeddings(\"cat\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_embeddings(df, folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    –ó–±–µ—Ä—ñ–≥–∞—î DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏ –≤ —É–∫–∞–∑–∞–Ω—É –ø–∞–ø–∫—É.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏\n",
    "        folder: –ù–∞–∑–≤–∞ –ø–∞–ø–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è\n",
    "        filename: –Ü–º'—è —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è\n",
    "    \"\"\"\n",
    "    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó, —è–∫—â–æ –≤–æ–Ω–∞ –Ω–µ —ñ—Å–Ω—É—î\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è DataFrame\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    \n",
    "    print(f\"DataFrame —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {file_path}\")\n",
    "\n",
    "def load_embeddings(folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏ –∑ —É–∫–∞–∑–∞–Ω–æ—ó –ø–∞–ø–∫–∏.\n",
    "    \n",
    "    Args:\n",
    "        folder: –ù–∞–∑–≤–∞ –ø–∞–ø–∫–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è\n",
    "        filename: –Ü–º'—è —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏ –∞–±–æ None, —è–∫—â–æ —Ñ–∞–π–ª –Ω–µ —ñ—Å–Ω—É—î\n",
    "    \"\"\"\n",
    "    # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É\n",
    "    if os.path.exists(file_path):\n",
    "        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è DataFrame\n",
    "        with open(file_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        \n",
    "        print(f\"DataFrame —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {file_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"–§–∞–π–ª {file_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ\")\n",
    "        return None\n",
    "\n",
    "def get_or_create_embeddings(df, chunk_column, embedding_function, folder=\"embeddings\", filename=\"flattened_df.pkl\"):\n",
    "    \"\"\"\n",
    "    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏ –∞–±–æ —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π.\n",
    "    \n",
    "    Args:\n",
    "        df: –í–∏—Ö—ñ–¥–Ω–∏–π DataFrame –∑ —Ç–µ–∫—Å—Ç–∞–º–∏\n",
    "        chunk_column: –ù–∞–∑–≤–∞ —Å—Ç–æ–≤–ø—Ü—è –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏\n",
    "        embedding_function: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤\n",
    "        folder: –ù–∞–∑–≤–∞ –ø–∞–ø–∫–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è/–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è\n",
    "        filename: –Ü–º'—è —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è/–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame –∑ –µ–º–±–µ–¥—ñ–Ω–≥–∞–º–∏\n",
    "    \"\"\"\n",
    "    # –°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ DataFrame\n",
    "    loaded_df = load_embeddings(folder, filename)\n",
    "    \n",
    "    if loaded_df is not None:\n",
    "        return loaded_df\n",
    "    \n",
    "    # –Ø–∫—â–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–µ –≤–¥–∞–ª–æ—Å—è, —Å—Ç–≤–æ—Ä—é—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥–∏\n",
    "    print(\"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤...\")\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤\n",
    "    embeddings = []\n",
    "    for chunk in df[chunk_column]:\n",
    "        embeddings.append(embedding_function(chunk))\n",
    "    \n",
    "    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ –≤ DataFrame\n",
    "    df['embeddings'] = embeddings\n",
    "    \n",
    "    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è DataFrame\n",
    "    save_embeddings(df, folder, filename)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é, —è–∫–∞ —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î –µ–º–±–µ–¥—ñ–Ω–≥–∏, \n",
    "# —è–∫—â–æ –≤–æ–Ω–∏ –Ω–µ –±—É–ª–∏ —Ä–∞–Ω—ñ—à–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ —ñ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ –ø–∞–ø—Ü—ñ –≤ \"15-rag-and-vector-databases/embeddings\"\n",
    "flattened_df = get_or_create_embeddings(\n",
    "    splitted_df.explode('chunks'), \n",
    "    'chunks', \n",
    "    create_embeddings\n",
    ")\n",
    "\n",
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ—à—É–∫ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º FAISS\n",
    "\n",
    "–í–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫ —Ç–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å –º—ñ–∂ –Ω–∞—à–∏–º –∑–∞–ø–∏—Ç–æ–º —ñ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É FAISS —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ –ø–æ—à—É–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ç—Ä–∏–º—É—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥–∏ —è–∫ –º–∞—Å–∏–≤ numpy\n",
    "embeddings_list = flattened_df['embeddings'].to_list()\n",
    "embeddings_array = np.array(embeddings_list).astype('float32')\n",
    "\n",
    "# –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤\n",
    "vector_dimension = len(embeddings_list[0])\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω–¥–µ–∫—Å FAISS\n",
    "index = faiss.IndexFlatL2(vector_dimension)  # L2 - —Ü–µ –µ–≤–∫–ª—ñ–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å\n",
    "\n",
    "# –î–æ–¥–∞—î–º–æ –Ω–∞—à—ñ –≤–µ–∫—Ç–æ—Ä–∏ –¥–æ —ñ–Ω–¥–µ–∫—Å—É\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤ –≤ —ñ–Ω–¥–µ–∫—Å—ñ\n",
    "print(f\"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤ –≤ —ñ–Ω–¥–µ–∫—Å—ñ: {index.ntotal}, —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤: {vector_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∞—à–µ —Ç–µ–∫—Å—Ç–æ–≤–µ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è\n",
    "question = \"–©–æ —Ç–∞–∫–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω?\"\n",
    "\n",
    "# –ü–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç–∞–Ω–Ω—è —É –≤–µ–∫—Ç–æ—Ä –∑–∞–ø–∏—Ç—É\n",
    "query_vector = create_embeddings(question)  \n",
    "query_vector_array = np.array([query_vector]).astype('float32')\n",
    "\n",
    "# –ó–Ω–∞–π–¥—ñ—Ç—å –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ (k=5 - —Å–∫—ñ–ª—å–∫–∏ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ —à—É–∫–∞—î–º–æ)\n",
    "k = 5\n",
    "distances, indices = index.search(query_vector_array, k)\n",
    "\n",
    "# –í–∏–≤–µ–¥—ñ—Ç—å –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏\n",
    "for i in range(min(3, len(indices[0]))):\n",
    "    idx = indices[0][i]\n",
    "    print(f\"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}:\")\n",
    "    print(flattened_df['chunks'].iloc[idx])\n",
    "    print(f\"–®–ª—è—Ö: {flattened_df['path'].iloc[idx]}\")\n",
    "    print(f\"–í—ñ–¥—Å—Ç–∞–Ω—å: {distances[0][i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ—î–¥–Ω–∞–Ω–Ω—è –≤—Å—å–æ–≥–æ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import ChatCompletionsToolDefinition\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "# –í–∏–±–µ—Ä—ñ—Ç—å –º–æ–¥–µ–ª—å –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ç–µ–∫—Å—Ç—É\n",
    "deployment = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —á–∞—Ç–±–æ—Ç—ñ–≤ (–ø—Ä–∏ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —ñ –≤—ñ–¥—Å—É—Ç–Ω–æ—Å—Ç—ñ RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_with_rag(user_input):\n",
    "    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç–∞–Ω–Ω—è —É –≤–µ–∫—Ç–æ—Ä –∑–∞–ø–∏—Ç—É\n",
    "    query_vector = create_embeddings(user_input)\n",
    "    query_vector_array = np.array([query_vector]).astype('float32')\n",
    "    \n",
    "    # –ó–Ω–∞–π–¥—ñ—Ç—å –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ FAISS\n",
    "    k = 5  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É\n",
    "    distances, indices = index.search(query_vector_array, k)\n",
    "\n",
    "    # –¥–æ–¥–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–æ –∑–∞–ø–∏—Ç—É, —â–æ–± –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "    history = []\n",
    "    for idx in indices[0]:\n",
    "        history.append(flattened_df['chunks'].iloc[idx])\n",
    "\n",
    "    # —Å—Ç–≤–æ—Ä—é—î–º–æ –æ–±'—î–∫—Ç –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "    context = \"\\n\\n\".join(history)  # –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏\n",
    "\n",
    "    # –§–æ—Ä–º—É—î–º–æ –∑–∞–ø–∏—Ç, —â–æ –ø—Ä–æ—Å–∏—Ç—å –∫–æ—Ä–æ—Ç–∫—É, –∞–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with AI questions. \"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_input}\\n\\n Provide a brief but complete answer based on the context. Answer in Ukrainian.\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = client.complete(\n",
    "        temperature=0,\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def chatbot_without_rag(user_input):\n",
    "    \"\"\"\n",
    "    –ß–∞—Ç-–±–æ—Ç –±–µ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è RAG (–ø—Ä—è–º–∏–π –∑–∞–ø–∏—Ç –¥–æ –º–æ–¥–µ–ª—ñ).\n",
    "    \"\"\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with AI questions. Provide brief but complete answers. Answer in Ukrainian.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {user_input}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.complete(\n",
    "        temperature=0,\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π RAG-—Å–∏—Å—Ç–µ–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "def compare_responses(user_input, save_to_file=False, filename=\"rag_comparison.md\"):\n",
    "    \"\"\"\n",
    "    –ü–æ—Ä—ñ–≤–Ω—é—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–∞—Ç-–±–æ—Ç—É –∑ RAG —Ç–∞ –±–µ–∑ RAG.\n",
    "    \n",
    "    Args:\n",
    "        user_input: –ó–∞–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞\n",
    "        save_to_file: –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–∞–π–ª Markdown\n",
    "        filename: –ù–∞–∑–≤–∞ —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è\n",
    "    \"\"\"\n",
    "    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤\n",
    "    query_vector = create_embeddings(user_input)\n",
    "    query_vector_array = np.array([query_vector]).astype('float32')\n",
    "    k = 5  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ –¥–ª—è –ø–æ—à—É–∫—É\n",
    "    distances, indices = index.search(query_vector_array, k)\n",
    "    \n",
    "    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π\n",
    "    rag_response = chatbot_with_rag(user_input)\n",
    "    no_rag_response = chatbot_without_rag(user_input)\n",
    "    \n",
    "    # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è markdown-—Ç–µ–∫—Å—Ç—É\n",
    "    markdown_text = f\"\"\"\n",
    "# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π\n",
    "\n",
    "## üìù –ó–∞–ø–∏—Ç: {user_input}\n",
    "\n",
    "## üîç –í—ñ–¥–ø–æ–≤—ñ–¥—ñ –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "### –ë–µ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è RAG\n",
    "\n",
    "{no_rag_response}\n",
    "\n",
    "### –ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º RAG\n",
    "\n",
    "{rag_response}\n",
    "\n",
    "## üìö –ó–Ω–∞–π–¥–µ–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ —Ç–µ–∫—Å—Ç—É\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # –î–æ–¥–∞–≤–∞–Ω–Ω—è —á–∞–Ω–∫—ñ–≤\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        chunk_content = flattened_df['chunks'].iloc[idx]\n",
    "        path = flattened_df['path'].iloc[idx]\n",
    "        dist = float(distances[0][i])\n",
    "        \n",
    "        markdown_text += f\"\"\"\n",
    "### –§—Ä–∞–≥–º–µ–Ω—Ç {i+1} (–≤—ñ–¥—Å—Ç–∞–Ω—å: {dist:.4f})\n",
    "\n",
    "**–®–ª—è—Ö**: {path}\n",
    "\n",
    "{chunk_content}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # –í–∏–≤–µ–¥–µ–Ω–Ω—è Markdown\n",
    "    display(Markdown(markdown_text))\n",
    "    \n",
    "    # –î–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º—É–ª\n",
    "    mathjax_script = \"\"\"\n",
    "    <script type=\"text/javascript\">\n",
    "        MathJax = {\n",
    "            tex: {\n",
    "                inlineMath: [['$', '$']]\n",
    "            }\n",
    "        };\n",
    "    </script>\n",
    "    <script type=\"text/javascript\" id=\"MathJax-script\" async\n",
    "        src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js\">\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    display(HTML(mathjax_script))\n",
    "    \n",
    "    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ —Ñ–∞–π–ª, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n",
    "    if save_to_file:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:\n",
    "compare_responses(\"–©–æ —Ç–∞–∫–µ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
