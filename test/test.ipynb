{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовий промпт\n",
    "Створимо базовий промпт для оптимізації паралельних обчислень на CUDA.\n",
    "Визначимо характерні риси які має мати наш промт:\n",
    "1. Чітке формулювання задачі: аби модель могла коректно зрозуміти наші вимоги.\n",
    "2. Використання ролі експерта: запит до GPT як до фахівця з CUDA аби надати йому роль в якій він буде давати відповідь.\n",
    "3. Конкретні вимоги: оптимізація пам’яті, синхронізація і т.д.\n",
    "4. Формат виводу: C++ з коментарями та поясненнями аби одразу отримати готовий результат в якому можна буде розібратись.\n",
    "\n",
    "Результат:\n",
    "\n",
    "\"You are an expert in GPU parallel computing and CUDA optimization. Your task is to generate an optimized CUDA kernel and corresponding host-side code for efficient parallel execution. The goal is to minimize execution time while ensuring correct synchronization and memory access patterns. The solution should include:\n",
    "An optimized CUDA kernel that efficiently utilizes shared memory, registers, and global memory access patterns.\n",
    "Proper thread block and grid size configuration for maximum performance.\n",
    "Techniques to avoid memory access bottlenecks, such as coalesced memory access and minimizing bank conflicts.\n",
    "Synchronization strategies to prevent race conditions and ensure correct results.\n",
    "A sample workload and benchmarking methodology to compare performance gains.\n",
    "Provide the implementation in C++ with CUDA and include explanations of key optimization choices. Format the response as well-structured code with comments and an explanation of the optimizations used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "#include <iostream>\n",
      "#include <cuda_runtime.h>\n",
      "\n",
      "#define N 1024 // Number of elements in the array\n",
      "#define BLOCK_SIZE 256 // Number of threads per block\n",
      "\n",
      "// CUDA kernel to perform element-wise addition of two arrays\n",
      "__global__ void vectorAddition(int* a, int* b, int* c)\n",
      "{\n",
      "    __shared__ int sharedMem[BLOCK_SIZE]; // Shared memory for each block\n",
      "    int tid = threadIdx.x;\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "\n",
      "    // Load data into shared memory\n",
      "    sharedMem[tid] = a[i] + b[i];\n",
      "    __syncthreads(); // Ensure all threads have finished loading data\n",
      "\n",
      "    // Perform addition using shared memory\n",
      "    for (int s = blockDim.x / 2; s > 0; s >>= 1)\n",
      "    {\n",
      "        if (tid < s)\n",
      "        {\n",
      "            sharedMem[tid] += sharedMem[tid + s];\n",
      "        }\n",
      "        __syncthreads(); // Ensure all threads have finished current iteration\n",
      "    }\n",
      "\n",
      "    // Write result back to global memory\n",
      "    if (tid == 0)\n",
      "    {\n",
      "        c[blockIdx.x] = sharedMem[0];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int *a, *b, *c; // Host arrays\n",
      "    int *d_a, *d_b, *d_c; // Device arrays\n",
      "\n",
      "    // Allocate memory on host\n",
      "    a = new int[N];\n",
      "    b = new int[N];\n",
      "    c = new int[N/BLOCK_SIZE];\n",
      "\n",
      "    // Initialize input arrays\n",
      "    for (int i = 0; i < N; i++)\n",
      "    {\n",
      "        a[i] = i;\n",
      "        b[i] = i * 2;\n",
      "    }\n",
      "\n",
      "    // Allocate memory on device\n",
      "    cudaMalloc(&d_a, N * sizeof(int));\n",
      "    cudaMalloc(&d_b, N * sizeof(int));\n",
      "    cudaMalloc(&d_c, (N/BLOCK_SIZE) * sizeof(int));\n",
      "\n",
      "    // Copy input data from host to device\n",
      "    cudaMemcpy(d_a, a, N * sizeof(int), cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, b, N * sizeof(int), cudaMemcpyHostToDevice);\n",
      "\n",
      "    // Configure thread block and grid size\n",
      "    dim3 blockSize(BLOCK_SIZE, 1, 1);\n",
      "    dim3 gridSize((N + BLOCK_SIZE - 1) / BLOCK_SIZE, 1, 1);\n",
      "\n",
      "    // Launch CUDA kernel\n",
      "    vectorAddition<<<gridSize, blockSize>>>(d_a, d_b, d_c);\n",
      "\n",
      "    // Copy output data from device to host\n",
      "    cudaMemcpy(c, d_c, (N/BLOCK_SIZE) * sizeof(int), cudaMemcpyDeviceToHost);\n",
      "\n",
      "    // Print output\n",
      "    for (int i = 0; i < N/BLOCK_SIZE; i++)\n",
      "    {\n",
      "        std::cout << c[i] << \" \";\n",
      "    }\n",
      "    std::cout << std::endl;\n",
      "\n",
      "    // Free memory\n",
      "    delete[] a;\n",
      "    delete[] b;\n",
      "    delete[] c;\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_c);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "In this optimized CUDA kernel for vector addition, we utilize shared memory to reduce global memory access and improve memory access patterns. Each block of threads loads a chunk of data into shared memory, performs addition using shared memory, and then writes the result back to global memory. This reduces memory access bottlenecks and improves performance.\n",
      "\n",
      "We configure the thread block size to be 256 threads, which is a multiple of 32 (warp size) to ensure efficient memory access and avoid bank conflicts. The grid size is calculated based on the number of elements in the array and the block size.\n",
      "\n",
      "We use proper synchronization techniques such as `__syncthreads()` to ensure correct synchronization between threads within a block. This prevents race conditions and ensures correct results.\n",
      "\n",
      "By optimizing memory access patterns, utilizing shared memory, and proper synchronization, we aim to minimize execution time and achieve efficient parallel execution.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable in your GitHub Codespace secrets.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "deployment=\"gpt-3.5-turbo\"\n",
    "\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "text = f\"\"\"\n",
    "You are an expert in GPU parallel computing and CUDA optimization. Your task is to generate an optimized CUDA kernel and corresponding host-side code for efficient parallel execution. The goal is to minimize execution time while ensuring correct synchronization and memory access patterns. The solution should include:\n",
    "An optimized CUDA kernel that efficiently utilizes shared memory, registers, and global memory access patterns.\n",
    "Proper thread block and grid size configuration for maximum performance.\n",
    "Techniques to avoid memory access bottlenecks, such as coalesced memory access and minimizing bank conflicts.\n",
    "Synchronization strategies to prevent race conditions and ensure correct results.\n",
    "A sample workload and benchmarking methodology to compare performance gains.\n",
    "Provide the implementation in C++ with CUDA and include explanations of key optimization choices. Format the response as well-structured code with comments and an explanation of the optimizations used.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проведення токенізації\n",
    "Візьмемо вже відомий нам код з першої комірки виконання базових вправ і проаналізуємо наш промпт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 2675, 527, 459, 6335, 304, 23501, 15638, 25213, 323, 55909, 26329, 13, 4718, 3465, 374, 311, 7068, 459, 34440, 55909, 10206, 323, 12435, 3552, 25034, 2082, 369, 11297, 15638, 11572, 13, 578, 5915, 374, 311, 30437, 11572, 892, 1418, 23391, 4495, 59012, 323, 5044, 2680, 12912, 13, 578, 6425, 1288, 2997, 512, 2127, 34440, 55909, 10206, 430, 30820, 60880, 6222, 5044, 11, 25771, 11, 323, 3728, 5044, 2680, 12912, 627, 1360, 716, 4617, 2565, 323, 5950, 1404, 6683, 369, 7340, 5178, 627, 29356, 8467, 311, 5766, 5044, 2680, 11176, 35291, 14895, 11, 1778, 439, 1080, 3916, 2041, 5044, 2680, 323, 77391, 6201, 26885, 627, 50, 69029, 15174, 311, 5471, 7102, 4787, 323, 6106, 4495, 3135, 627, 32, 6205, 54696, 323, 29531, 287, 38152, 311, 9616, 5178, 20192, 627, 61524, 279, 8292, 304, 356, 1044, 449, 55909, 323, 2997, 41941, 315, 1401, 26329, 11709, 13, 15392, 279, 2077, 439, 1664, 12, 52243, 2082, 449, 6170, 323, 459, 16540, 315, 279, 82278, 1511, 627]\n",
      "Number of tokens: 163\n",
      "[b'\\n', b'You', b' are', b' an', b' expert', b' in', b' GPU', b' parallel', b' computing', b' and', b' CUDA', b' optimization', b'.', b' Your', b' task', b' is', b' to', b' generate', b' an', b' optimized', b' CUDA', b' kernel', b' and', b' corresponding', b' host', b'-side', b' code', b' for', b' efficient', b' parallel', b' execution', b'.', b' The', b' goal', b' is', b' to', b' minimize', b' execution', b' time', b' while', b' ensuring', b' correct', b' synchronization', b' and', b' memory', b' access', b' patterns', b'.', b' The', b' solution', b' should', b' include', b':\\n', b'An', b' optimized', b' CUDA', b' kernel', b' that', b' efficiently', b' utilizes', b' shared', b' memory', b',', b' registers', b',', b' and', b' global', b' memory', b' access', b' patterns', b'.\\n', b'Pro', b'per', b' thread', b' block', b' and', b' grid', b' size', b' configuration', b' for', b' maximum', b' performance', b'.\\n', b'Techn', b'iques', b' to', b' avoid', b' memory', b' access', b' bott', b'lene', b'cks', b',', b' such', b' as', b' co', b'ales', b'ced', b' memory', b' access', b' and', b' minimizing', b' bank', b' conflicts', b'.\\n', b'S', b'ynchronization', b' strategies', b' to', b' prevent', b' race', b' conditions', b' and', b' ensure', b' correct', b' results', b'.\\n', b'A', b' sample', b' workload', b' and', b' benchmark', b'ing', b' methodology', b' to', b' compare', b' performance', b' gains', b'.\\n', b'Provide', b' the', b' implementation', b' in', b' C', b'++', b' with', b' CUDA', b' and', b' include', b' explanations', b' of', b' key', b' optimization', b' choices', b'.', b' Format', b' the', b' response', b' as', b' well', b'-', b'structured', b' code', b' with', b' comments', b' and', b' an', b' explanation', b' of', b' the', b' optimizations', b' used', b'.\\n']\n",
      "Average length of tokens: 5.99\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "You are an expert in GPU parallel computing and CUDA optimization. Your task is to generate an optimized CUDA kernel and corresponding host-side code for efficient parallel execution. The goal is to minimize execution time while ensuring correct synchronization and memory access patterns. The solution should include:\n",
    "An optimized CUDA kernel that efficiently utilizes shared memory, registers, and global memory access patterns.\n",
    "Proper thread block and grid size configuration for maximum performance.\n",
    "Techniques to avoid memory access bottlenecks, such as coalesced memory access and minimizing bank conflicts.\n",
    "Synchronization strategies to prevent race conditions and ensure correct results.\n",
    "A sample workload and benchmarking methodology to compare performance gains.\n",
    "Provide the implementation in C++ with CUDA and include explanations of key optimization choices. Format the response as well-structured code with comments and an explanation of the optimizations used.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "decoded_tokens = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
    "print(decoded_tokens)\n",
    "\n",
    "# Calculate the average length of tokens\n",
    "total_length = sum(len(token) for token in decoded_tokens)\n",
    "average_length = total_length / len(decoded_tokens) if len(decoded_tokens) > 0 else 0\n",
    "\n",
    "# Print the average length of tokens\n",
    "print(f\"Average length of tokens: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцінка якості результатів\n",
    "\n",
    "Оптимальне використання shared memory, правильна синхронізація, ефективний розмір блоків, коректне керування пам’яттю. Відповідь дана на мові C++, коментарі присутні, в цілому наш промт був повністю виконаний. Що до оптимальності результатів роботи самого коду можна було б дати відповідь тільки самому будучи експертом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запропонувати покращення\n",
    "\n",
    "На основі отриманих результатів та аналізу токенізації, ми можемо запропонувати наступні покращення для оптимізації паралельних обчислень на CUDA:\n",
    "- Використання більш специфічних інструкцій для моделі.\n",
    "- Додавання додаткових деталей про архітектуру CUDA та можливості паралельних обчислень.\n",
    "- Розробка більш складних промптів для врахування різних сценаріїв паралельних обчислень."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
